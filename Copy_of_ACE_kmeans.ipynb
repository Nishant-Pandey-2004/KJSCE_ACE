{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RPDjSrSQuW-u"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Rajesh pandey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import silhouette_score\n",
        "from prophet import Prophet\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# 1. Data Loading and Initial Preprocessing\n",
        "def load_and_preprocess_data(file_path):\n",
        "    \"\"\"\n",
        "    Load and preprocess the retail dataset\n",
        "    \"\"\"\n",
        "    # Load data\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Convert Order Date to datetime\n",
        "    df['Order Date'] = pd.to_datetime(df['Order Date'], format='%d-%m-%Y')\n",
        "\n",
        "    # Create time-based features\n",
        "    df['Year'] = df['Order Date'].dt.year\n",
        "    df['Month'] = df['Order Date'].dt.month\n",
        "    df['Day of Week'] = df['Order Date'].dt.dayofweek\n",
        "    df['Is Weekend'] = df['Day of Week'].isin([5, 6]).astype(int)\n",
        "\n",
        "    # Calculate customer metrics\n",
        "    customer_metrics = calculate_customer_metrics(df)\n",
        "    df = df.merge(customer_metrics, on='Customer Name')\n",
        "\n",
        "    return df\n",
        "\n",
        "def calculate_customer_metrics(df):\n",
        "    \"\"\"\n",
        "    Calculate RFM metrics for each customer\n",
        "    \"\"\"\n",
        "    # Current date for recency calculation\n",
        "    current_date = df['Order Date'].max()\n",
        "\n",
        "    customer_metrics = df.groupby('Customer Name').agg({\n",
        "        'Order Date': lambda x: (current_date - x.max()).days,  # Recency\n",
        "        'Order ID': 'count',  # Frequency\n",
        "        'Sales': 'sum'  # Monetary\n",
        "    }).reset_index()\n",
        "\n",
        "    customer_metrics.columns = ['Customer Name', 'Recency', 'Frequency', 'Total_Monetary']\n",
        "    return customer_metrics\n",
        "\n",
        "# 2. Customer Segmentation\n",
        "def perform_customer_segmentation(df, n_clusters=4):\n",
        "    \"\"\"\n",
        "    Perform customer segmentation using RFM metrics\n",
        "    \"\"\"\n",
        "    # Select features for clustering\n",
        "    features = ['Recency', 'Frequency', 'Total_Monetary']\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    features_scaled = scaler.fit_transform(df[features])\n",
        "\n",
        "    # Find optimal number of clusters\n",
        "    silhouette_scores = []\n",
        "    K = range(2, 8)\n",
        "\n",
        "    for k in K:\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "        kmeans.fit(features_scaled)\n",
        "        score = silhouette_score(features_scaled, kmeans.labels_)\n",
        "        silhouette_scores.append(score)\n",
        "\n",
        "    optimal_k = K[np.argmax(silhouette_scores)]\n",
        "\n",
        "    # Perform final clustering\n",
        "    kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "    df['Customer_Segment'] = kmeans.fit_predict(features_scaled)\n",
        "\n",
        "    return df, kmeans\n",
        "\n",
        "# 3. Sales Forecasting\n",
        "def create_sales_forecast(df, forecast_periods=30):\n",
        "    \"\"\"\n",
        "    Create sales forecast using Prophet\n",
        "    \"\"\"\n",
        "    # Prepare data for Prophet\n",
        "    daily_sales = df.groupby('Order Date')['Sales'].sum().reset_index()\n",
        "    daily_sales.columns = ['ds', 'y']\n",
        "\n",
        "    # Initialize and train Prophet model\n",
        "    model = Prophet(\n",
        "        yearly_seasonality=True,\n",
        "        weekly_seasonality=True,\n",
        "        daily_seasonality=False,\n",
        "        seasonality_mode='multiplicative'\n",
        "    )\n",
        "\n",
        "    # Add holiday effects\n",
        "    model.add_country_holidays(country_name='IN')\n",
        "\n",
        "    # Fit model\n",
        "    model.fit(daily_sales)\n",
        "\n",
        "    # Create future dates dataframe\n",
        "    future_dates = model.make_future_dataframe(periods=forecast_periods)\n",
        "\n",
        "    # Generate forecast\n",
        "    forecast = model.predict(future_dates)\n",
        "\n",
        "    return model, forecast\n",
        "\n",
        "# 4. Store Placement Analysis\n",
        "def analyze_store_placement(df):\n",
        "    \"\"\"\n",
        "    Analyze optimal store placement based on various metrics\n",
        "    \"\"\"\n",
        "    store_analysis = df.groupby(['Region', 'City']).agg({\n",
        "        'Sales': 'sum',\n",
        "        'Profit': 'sum',\n",
        "        'Order ID': 'count',\n",
        "        'Customer Name': 'nunique',\n",
        "        'Discount': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    store_analysis['Sales_per_Customer'] = store_analysis['Sales'] / store_analysis['Customer Name']\n",
        "    store_analysis['Profit_Margin'] = store_analysis['Profit'] / store_analysis['Sales']\n",
        "\n",
        "    # Create composite score\n",
        "    weights = {\n",
        "        'Sales': 0.3,\n",
        "        'Profit_Margin': 0.3,\n",
        "        'Customer Name': 0.2,\n",
        "        'Sales_per_Customer': 0.2\n",
        "    }\n",
        "\n",
        "    # Normalize metrics\n",
        "    for metric in weights.keys():\n",
        "        if metric in store_analysis.columns:\n",
        "            store_analysis[f'{metric}_Normalized'] = (\n",
        "                store_analysis[metric] - store_analysis[metric].min()\n",
        "            ) / (store_analysis[metric].max() - store_analysis[metric].min())\n",
        "\n",
        "    # Calculate final score\n",
        "    store_analysis['Location_Score'] = sum(\n",
        "        store_analysis[f'{metric}_Normalized'] * weight\n",
        "        for metric, weight in weights.items()\n",
        "    )\n",
        "\n",
        "    return store_analysis\n",
        "\n",
        "# 5. Product Recommendation System\n",
        "def build_product_recommendations(df):\n",
        "    \"\"\"\n",
        "    Build a product recommendation system based on customer purchase patterns\n",
        "    \"\"\"\n",
        "    # Create customer-product purchase matrix\n",
        "    purchase_matrix = pd.crosstab(\n",
        "        df['Customer Name'],\n",
        "        df['Category']\n",
        "    )\n",
        "\n",
        "    # Train a model to predict product preferences\n",
        "    X = purchase_matrix.values\n",
        "    y = df.groupby('Customer Name')['Sales'].mean().values\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
        "    y_train, y_test = train_test_split(y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train model\n",
        "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "\n",
        "    return rf_model, purchase_matrix\n",
        "\n",
        "# 6. Main Analysis Pipeline\n",
        "def run_retail_analysis(file_path):\n",
        "    \"\"\"\n",
        "    Run the complete retail analysis pipeline\n",
        "    \"\"\"\n",
        "    # 1. Load and preprocess data\n",
        "    print(\"Loading and preprocessing data...\")\n",
        "    df = load_and_preprocess_data(file_path)\n",
        "\n",
        "    # 2. Perform customer segmentation\n",
        "    print(\"Performing customer segmentation...\")\n",
        "    df, kmeans_model = perform_customer_segmentation(df)\n",
        "\n",
        "    # 3. Create sales forecast\n",
        "    print(\"Creating sales forecast...\")\n",
        "    prophet_model, forecast = create_sales_forecast(df)\n",
        "\n",
        "    # 4. Analyze store placement\n",
        "    print(\"Analyzing store placement...\")\n",
        "    store_analysis = analyze_store_placement(df)\n",
        "\n",
        "    # 5. Build product recommendations\n",
        "    print(\"Building product recommendations...\")\n",
        "    rec_model, purchase_matrix = build_product_recommendations(df)\n",
        "\n",
        "    return {\n",
        "        'processed_data': df,\n",
        "        'segmentation_model': kmeans_model,\n",
        "        'forecast_model': prophet_model,\n",
        "        'forecast_results': forecast,\n",
        "        'store_analysis': store_analysis,\n",
        "        'recommendation_model': rec_model,\n",
        "        'purchase_matrix': purchase_matrix\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JJVDKbr8vANS"
      },
      "outputs": [],
      "source": [
        "def standardize_date_format(df, date_column='Order Date'):\n",
        "    \"\"\"\n",
        "    Convert various date formats to standard 'dd-mm-yyyy' format\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): Input dataframe\n",
        "    date_column (str): Name of the date column to standardize\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: DataFrame with standardized date format\n",
        "    \"\"\"\n",
        "    def convert_date(date_str):\n",
        "        try:\n",
        "            # First try the mm/dd/yyyy format\n",
        "            if isinstance(date_str, str) and '/' in date_str:\n",
        "                date_obj = datetime.strptime(date_str, '%m/%d/%Y')\n",
        "                return date_obj.strftime('%d-%m-%Y')\n",
        "\n",
        "            # Try dd-mm-yyyy format\n",
        "            elif isinstance(date_str, str) and '-' in date_str:\n",
        "                # Verify if it's already in correct format\n",
        "                datetime.strptime(date_str, '%d-%m-%Y')\n",
        "                return date_str\n",
        "\n",
        "            # If date is already a datetime object\n",
        "            elif isinstance(date_str, datetime):\n",
        "                return date_str.strftime('%d-%m-%Y')\n",
        "\n",
        "            else:\n",
        "                # Try parsing with pandas (handles more formats)\n",
        "                return pd.to_datetime(date_str).strftime('%d-%m-%Y')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error converting date '{date_str}': {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    # Create a copy of the dataframe to avoid modifying the original\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Convert dates\n",
        "    df_copy[date_column] = df_copy[date_column].apply(convert_date)\n",
        "\n",
        "    # Remove any rows where date conversion failed\n",
        "    invalid_dates = df_copy[date_column].isna()\n",
        "    if invalid_dates.any():\n",
        "        print(f\"Warning: {invalid_dates.sum()} dates could not be converted and will be removed\")\n",
        "        df_copy = df_copy.dropna(subset=[date_column])\n",
        "\n",
        "    return df_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mL6To-MlvIPb"
      },
      "outputs": [],
      "source": [
        "# Load your data\n",
        "df = pd.read_csv('DMart_Grocery_Sales_-_Retail_Analytics_Dataset.csv')\n",
        "\n",
        "# Convert dates\n",
        "df_standardized = standardize_date_format(df, date_column='Order Date')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MYaVpANvwMQV"
      },
      "outputs": [],
      "source": [
        "def run_retail_analysis(df):\n",
        "    \"\"\"\n",
        "    Run the complete retail analysis pipeline\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): Input dataframe with standardized date format\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Initial preprocessing\n",
        "        print(\"Preprocessing data...\")\n",
        "        processed_df = df.copy()\n",
        "\n",
        "        # Convert Order Date to datetime if it isn't already\n",
        "        processed_df['Order Date'] = pd.to_datetime(processed_df['Order Date'], format='%d-%m-%Y')\n",
        "\n",
        "        # Create time-based features\n",
        "        processed_df['Year'] = processed_df['Order Date'].dt.year\n",
        "        processed_df['Month'] = processed_df['Order Date'].dt.month\n",
        "        processed_df['Day of Week'] = processed_df['Order Date'].dt.dayofweek\n",
        "        processed_df['Is Weekend'] = processed_df['Day of Week'].isin([5, 6]).astype(int)\n",
        "\n",
        "        # 2. Calculate customer metrics\n",
        "        print(\"Calculating customer metrics...\")\n",
        "        current_date = processed_df['Order Date'].max()\n",
        "\n",
        "        customer_metrics = processed_df.groupby('Customer Name').agg({\n",
        "            'Order Date': lambda x: (current_date - x.max()).days,  # Recency\n",
        "            'Order ID': 'count',  # Frequency\n",
        "            'Sales': 'sum'  # Monetary\n",
        "        }).reset_index()\n",
        "\n",
        "        customer_metrics.columns = ['Customer Name', 'Recency', 'Frequency', 'Total_Monetary']\n",
        "        processed_df = processed_df.merge(customer_metrics, on='Customer Name')\n",
        "\n",
        "        # 3. Customer Segmentation\n",
        "        print(\"Performing customer segmentation...\")\n",
        "        features = ['Recency', 'Frequency', 'Total_Monetary']\n",
        "        scaler = StandardScaler()\n",
        "        features_scaled = scaler.fit_transform(customer_metrics[features])\n",
        "\n",
        "        kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "        customer_metrics['Customer_Segment'] = kmeans.fit_predict(features_scaled)\n",
        "        processed_df = processed_df.merge(\n",
        "            customer_metrics[['Customer Name', 'Customer_Segment']],\n",
        "            on='Customer Name'\n",
        "        )\n",
        "\n",
        "        # 4. Sales Forecasting\n",
        "        print(\"Creating sales forecast...\")\n",
        "        daily_sales = processed_df.groupby('Order Date')['Sales'].sum().reset_index()\n",
        "        daily_sales.columns = ['ds', 'y']\n",
        "\n",
        "        prophet_model = Prophet(\n",
        "            yearly_seasonality=True,\n",
        "            weekly_seasonality=True,\n",
        "            daily_seasonality=False\n",
        "        )\n",
        "        prophet_model.fit(daily_sales)\n",
        "\n",
        "        future_dates = prophet_model.make_future_dataframe(periods=30)\n",
        "        forecast = prophet_model.predict(future_dates)\n",
        "\n",
        "        # 5. Store Placement Analysis\n",
        "        print(\"Analyzing store placement...\")\n",
        "        store_analysis = processed_df.groupby(['Region', 'City']).agg({\n",
        "            'Sales': 'sum',\n",
        "            'Profit': 'sum',\n",
        "            'Order ID': 'count',\n",
        "            'Customer Name': 'nunique',\n",
        "            'Discount': 'mean'\n",
        "        }).reset_index()\n",
        "\n",
        "        # Calculate performance metrics\n",
        "        store_analysis['Sales_per_Customer'] = store_analysis['Sales'] / store_analysis['Customer Name']\n",
        "        store_analysis['Profit_Margin'] = store_analysis['Profit'] / store_analysis['Sales']\n",
        "\n",
        "        # 6. Product Recommendations\n",
        "        print(\"Building product recommendations...\")\n",
        "        purchase_matrix = pd.crosstab(\n",
        "            processed_df['Customer Name'],\n",
        "            processed_df['Category']\n",
        "        )\n",
        "\n",
        "        X = purchase_matrix.values\n",
        "        y = processed_df.groupby('Customer Name')['Sales'].mean().values\n",
        "\n",
        "        rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "        rf_model.fit(X, y)\n",
        "\n",
        "        return {\n",
        "            'processed_data': processed_df,\n",
        "            'segmentation_model': kmeans,\n",
        "            'forecast_model': prophet_model,\n",
        "            'forecast_results': forecast,\n",
        "            'store_analysis': store_analysis,\n",
        "            'recommendation_model': rf_model,\n",
        "            'purchase_matrix': purchase_matrix\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in analysis pipeline: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Function to display key insights\n",
        "def display_insights(results):\n",
        "    \"\"\"\n",
        "    Display key insights from the analysis\n",
        "    \"\"\"\n",
        "    processed_data = results['processed_data']\n",
        "    forecast = results['forecast_results']\n",
        "    store_analysis = results['store_analysis']\n",
        "\n",
        "    print(\"\\n=== ANALYSIS INSIGHTS ===\")\n",
        "\n",
        "    # Customer Segments\n",
        "    print(\"\\nCustomer Segments:\")\n",
        "    segment_stats = processed_data.groupby('Customer_Segment').agg({\n",
        "        'Customer Name': 'nunique',\n",
        "        'Sales': 'mean',\n",
        "        'Frequency': 'mean'\n",
        "    }).round(2)\n",
        "    print(segment_stats)\n",
        "\n",
        "    # Top Performing Regions\n",
        "    print(\"\\nTop Performing Regions:\")\n",
        "    top_regions = store_analysis.groupby('Region')['Sales'].sum().sort_values(ascending=False)\n",
        "    print(top_regions)\n",
        "\n",
        "    # Sales Forecast\n",
        "    print(\"\\nSales Forecast (Next 30 days):\")\n",
        "    forecast_summary = forecast.tail(30)[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].describe()\n",
        "    print(forecast_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgM3rXOWubBe",
        "outputId": "93bd8370-80a8-49f2-8803-12a8039ce027"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing data...\n",
            "Calculating customer metrics...\n",
            "Performing customer segmentation...\n",
            "Creating sales forecast...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "07:18:55 - cmdstanpy - INFO - Chain [1] start processing\n",
            "07:18:55 - cmdstanpy - INFO - Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing store placement...\n",
            "Building product recommendations...\n",
            "\n",
            "=== ANALYSIS INSIGHTS ===\n",
            "\n",
            "Customer Segments:\n",
            "                  Customer Name    Sales  Frequency\n",
            "Customer_Segment                                   \n",
            "0                            11  1482.76     201.54\n",
            "1                            21  1509.05     198.69\n",
            "2                             9  1501.12     215.89\n",
            "3                             9  1478.61     185.60\n",
            "\n",
            "Top Performing Regions:\n",
            "Region\n",
            "West       4798743\n",
            "East       4248368\n",
            "Central    3468156\n",
            "South      2440461\n",
            "North         1254\n",
            "Name: Sales, dtype: int64\n",
            "\n",
            "Sales Forecast (Next 30 days):\n",
            "                        ds          yhat   yhat_lower    yhat_upper\n",
            "count                   30     30.000000    30.000000     30.000000\n",
            "mean   2019-01-14 12:00:00  15083.733862  3827.456787  26352.765114\n",
            "min    2018-12-31 00:00:00  10112.243670 -1015.434634  20562.915038\n",
            "25%    2019-01-07 06:00:00  13055.558048  1735.068912  25154.891569\n",
            "50%    2019-01-14 12:00:00  14567.264777  3397.459246  25720.081234\n",
            "75%    2019-01-21 18:00:00  17459.721270  5799.871180  28357.358915\n",
            "max    2019-01-29 00:00:00  21388.282579  9777.268923  32768.540433\n",
            "std                    NaN   2880.253656  2757.343682   2849.203302\n",
            "\n",
            "Processed Data Sample:\n",
            "  Order ID Customer Name          Category      Sub Category         City  \\\n",
            "0      OD1        Harish      Oil & Masala           Masalas      Vellore   \n",
            "1      OD2         Sudha         Beverages     Health Drinks  Krishnagiri   \n",
            "2      OD3       Hussain       Food Grains      Atta & Flour   Perambalur   \n",
            "3      OD4       Jackson  Fruits & Veggies  Fresh Vegetables   Dharmapuri   \n",
            "4      OD5       Ridhesh       Food Grains   Organic Staples         Ooty   \n",
            "\n",
            "  Order Date Region  Sales  Discount  Profit       State  Year  Month  \\\n",
            "0 2017-08-11  North   1254      0.12  401.28  Tamil Nadu  2017      8   \n",
            "1 2017-08-11  South    749      0.18  149.80  Tamil Nadu  2017      8   \n",
            "2 2017-12-06   West   2360      0.21  165.20  Tamil Nadu  2017     12   \n",
            "3 2016-11-10  South    896      0.25   89.60  Tamil Nadu  2016     11   \n",
            "4 2016-11-10  South   2355      0.26  918.45  Tamil Nadu  2016     11   \n",
            "\n",
            "   Day of Week  Is Weekend  Recency  Frequency  Total_Monetary  \\\n",
            "0            4           0        7        208          293839   \n",
            "1            4           0        9        189          273493   \n",
            "2            2           0        0        208          307337   \n",
            "3            3           0        6        182          271793   \n",
            "4            3           0        2        204          309639   \n",
            "\n",
            "   Customer_Segment  \n",
            "0                 0  \n",
            "1                 3  \n",
            "2                 1  \n",
            "3                 3  \n",
            "4                 1  \n",
            "\n",
            "Next Week's Sales Forecast:\n",
            "             ds          yhat   yhat_lower    yhat_upper\n",
            "1259 2019-01-23  11759.591910   652.479577  22487.154368\n",
            "1260 2019-01-24  10284.135611  -496.768164  21230.604845\n",
            "1261 2019-01-25  12499.959009   873.943471  24115.529201\n",
            "1262 2019-01-26  14444.348009  3135.086964  25142.173499\n",
            "1263 2019-01-27  14386.419876  3656.218744  25889.515050\n",
            "1264 2019-01-28  14739.449869  3852.108431  25354.726910\n",
            "1265 2019-01-29  16158.326493  4233.304011  27905.766296\n",
            "\n",
            "Top Performing Stores:\n",
            "   Region         City   Sales  Profit_Margin\n",
            "73   West         Bodi  247873       0.264347\n",
            "90   West      Tenkasi  225701       0.244395\n",
            "94   West      Vellore  225617       0.246069\n",
            "77   West   Dharmapuri  220077       0.251272\n",
            "92   West  Tirunelveli  219492       0.246313\n"
          ]
        }
      ],
      "source": [
        "# First, standardize your dates\n",
        "df_standardized = standardize_date_format(df, date_column='Order Date')\n",
        "\n",
        "# Run the complete analysis\n",
        "try:\n",
        "    results = run_retail_analysis(df_standardized)\n",
        "\n",
        "    # Display insights\n",
        "    display_insights(results)\n",
        "\n",
        "    # Access individual components\n",
        "    processed_data = results['processed_data']\n",
        "    forecast = results['forecast_results']\n",
        "    store_analysis = results['store_analysis']\n",
        "\n",
        "    # Example: Print the first few rows of processed data\n",
        "    print(\"\\nProcessed Data Sample:\")\n",
        "    print(processed_data.head())\n",
        "\n",
        "    # Example: Print forecast for next week\n",
        "    print(\"\\nNext Week's Sales Forecast:\")\n",
        "    print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(7))\n",
        "\n",
        "    # Example: Print top performing stores\n",
        "    print(\"\\nTop Performing Stores:\")\n",
        "    print(store_analysis.nlargest(5, 'Sales')[['Region', 'City', 'Sales', 'Profit_Margin']])\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error running analysis: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jXvL8nOUvZKc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Personalized Recommendations for Customer in Krishnagiri, South\n",
            "\n",
            "1. Top 5 Recommended Categories Based on Your Shopping Pattern:\n",
            "================================================================================\n",
            "1. Category: Bakery               Sub-Category: Biscuits            \n",
            "   Recommendation Score: 0.83\n",
            "4. Category: Beverages            Sub-Category: Health Drinks       \n",
            "   Recommendation Score: 0.82\n",
            "5. Category: Beverages            Sub-Category: Soft Drinks         \n",
            "   Recommendation Score: 0.79\n",
            "9. Category: Eggs, Meat & Fish    Sub-Category: Mutton              \n",
            "   Recommendation Score: 0.75\n",
            "18. Category: Oil & Masala         Sub-Category: Edible Oil & Ghee   \n",
            "   Recommendation Score: 0.75\n",
            "\n",
            "2. New Categories Popular in Your Region:\n",
            "================================================================================\n",
            "\n",
            "3. Your Shopping Pattern Summary:\n",
            "================================================================================\n",
            "Category: Bakery               Sub-Category: Biscuits            \n",
            "Orders:  13, Average Spend: ₹1,476.15, Typical Discount: 26.7%\n",
            "Category: Bakery               Sub-Category: Breads & Buns       \n",
            "Orders:   9, Average Spend: ₹1,546.67, Typical Discount: 24.9%\n",
            "Category: Bakery               Sub-Category: Cakes               \n",
            "Orders:   7, Average Spend: ₹1,550.57, Typical Discount: 21.6%\n",
            "Category: Beverages            Sub-Category: Health Drinks       \n",
            "Orders:  12, Average Spend: ₹1,127.67, Typical Discount: 23.2%\n",
            "Category: Beverages            Sub-Category: Soft Drinks         \n",
            "Orders:   9, Average Spend: ₹1,551.78, Typical Discount: 21.9%\n",
            "Category: Eggs, Meat & Fish    Sub-Category: Chicken             \n",
            "Orders:   5, Average Spend: ₹1,287.80, Typical Discount: 26.2%\n",
            "Category: Eggs, Meat & Fish    Sub-Category: Eggs                \n",
            "Orders:   6, Average Spend: ₹983.00, Typical Discount: 24.8%\n",
            "Category: Eggs, Meat & Fish    Sub-Category: Fish                \n",
            "Orders:  10, Average Spend: ₹1,431.30, Typical Discount: 21.0%\n",
            "Category: Eggs, Meat & Fish    Sub-Category: Mutton              \n",
            "Orders:   9, Average Spend: ₹1,728.78, Typical Discount: 22.3%\n",
            "Category: Food Grains          Sub-Category: Atta & Flour        \n",
            "Orders:   5, Average Spend: ₹1,495.60, Typical Discount: 23.0%\n",
            "Category: Food Grains          Sub-Category: Dals & Pulses       \n",
            "Orders:   7, Average Spend: ₹1,602.00, Typical Discount: 23.1%\n",
            "Category: Food Grains          Sub-Category: Organic Staples     \n",
            "Orders:   9, Average Spend: ₹1,444.78, Typical Discount: 23.6%\n",
            "Category: Food Grains          Sub-Category: Rice                \n",
            "Orders:   8, Average Spend: ₹1,471.38, Typical Discount: 21.8%\n",
            "Category: Fruits & Veggies     Sub-Category: Fresh Fruits        \n",
            "Orders:   9, Average Spend: ₹1,468.00, Typical Discount: 22.2%\n",
            "Category: Fruits & Veggies     Sub-Category: Fresh Vegetables    \n",
            "Orders:   6, Average Spend: ₹1,476.67, Typical Discount: 23.5%\n",
            "Category: Fruits & Veggies     Sub-Category: Organic Fruits      \n",
            "Orders:   7, Average Spend: ₹1,493.71, Typical Discount: 22.6%\n",
            "Category: Fruits & Veggies     Sub-Category: Organic Vegetables  \n",
            "Orders:   4, Average Spend: ₹1,911.75, Typical Discount: 21.0%\n",
            "Category: Oil & Masala         Sub-Category: Edible Oil & Ghee   \n",
            "Orders:   8, Average Spend: ₹1,778.25, Typical Discount: 18.1%\n",
            "Category: Oil & Masala         Sub-Category: Masalas             \n",
            "Orders:   9, Average Spend: ₹1,343.78, Typical Discount: 21.9%\n",
            "Category: Oil & Masala         Sub-Category: Spices              \n",
            "Orders:   9, Average Spend: ₹1,795.89, Typical Discount: 25.0%\n",
            "Category: Snacks               Sub-Category: Chocolates          \n",
            "Orders:   9, Average Spend: ₹1,218.11, Typical Discount: 21.0%\n",
            "Category: Snacks               Sub-Category: Cookies             \n",
            "Orders:  10, Average Spend: ₹1,160.90, Typical Discount: 25.9%\n",
            "Category: Snacks               Sub-Category: Noodles             \n",
            "Orders:   9, Average Spend: ₹1,236.22, Typical Discount: 27.3%\n"
          ]
        }
      ],
      "source": [
        "def generate_personalized_recommendations(df, customer_name='Harish'):\n",
        "    \"\"\"\n",
        "    Generate personalized product recommendations for a DMart customer based on their purchase history,\n",
        "    regional preferences, and sub-category analysis\n",
        "    \n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): DataFrame with columns [Order ID, Customer Name, Category, Sub Category, \n",
        "                          City, Order Date, Region, Sales, Discount, Profit, State]\n",
        "    customer_name (str): Name of the customer to generate recommendations for\n",
        "    \n",
        "    Returns:\n",
        "    dict: Dictionary containing recommendations and supporting data\n",
        "    \"\"\"\n",
        "    # Get customer's purchase history\n",
        "    customer_purchases = df[df['Customer Name'] == customer_name]\n",
        "    \n",
        "    if len(customer_purchases) == 0:\n",
        "        return {\"error\": f\"Customer {customer_name} not found in the dataset\"}\n",
        "    \n",
        "    # Get customer's region and city\n",
        "    customer_region = customer_purchases['Region'].iloc[0]\n",
        "    customer_city = customer_purchases['City'].iloc[0]\n",
        "    \n",
        "    # Calculate customer's category and sub-category preferences\n",
        "    category_preferences = customer_purchases.groupby(['Category', 'Sub Category']).agg({\n",
        "        'Order ID': 'count',\n",
        "        'Sales': 'sum',\n",
        "        'Profit': 'sum',\n",
        "        'Discount': 'mean'\n",
        "    }).reset_index()\n",
        "    \n",
        "    category_preferences['Purchase_Frequency'] = category_preferences['Order ID']\n",
        "    category_preferences['Avg_Spend'] = category_preferences['Sales'] / category_preferences['Order ID']\n",
        "    category_preferences['Profit_Margin'] = category_preferences['Profit'] / category_preferences['Sales']\n",
        "    \n",
        "    # Get regional preferences (from same region)\n",
        "    regional_preferences = df[df['Region'] == customer_region].groupby(['Category', 'Sub Category']).agg({\n",
        "        'Order ID': 'count',\n",
        "        'Sales': 'sum',\n",
        "        'Customer Name': 'nunique',\n",
        "        'Profit': 'sum'\n",
        "    }).reset_index()\n",
        "    \n",
        "    regional_preferences['Regional_Popularity'] = regional_preferences['Order ID']\n",
        "    regional_preferences['Customer_Base'] = regional_preferences['Customer Name']\n",
        "    regional_preferences['Regional_Profit_Margin'] = regional_preferences['Profit'] / regional_preferences['Sales']\n",
        "    \n",
        "    # Merge customer and regional preferences\n",
        "    recommendations = category_preferences.merge(\n",
        "        regional_preferences[['Category', 'Sub Category', 'Regional_Popularity', \n",
        "                            'Customer_Base', 'Regional_Profit_Margin']], \n",
        "        on=['Category', 'Sub Category'], \n",
        "        how='outer'\n",
        "    ).fillna(0)\n",
        "    \n",
        "    # Calculate recommendation score\n",
        "    recommendations['Score'] = (\n",
        "        0.35 * (recommendations['Purchase_Frequency'] / recommendations['Purchase_Frequency'].max()) +  # Personal preference\n",
        "        0.25 * (recommendations['Avg_Spend'] / recommendations['Avg_Spend'].max()) +                   # Spending pattern\n",
        "        0.20 * (recommendations['Regional_Popularity'] / recommendations['Regional_Popularity'].max()) + # Regional popularity\n",
        "        0.20 * (1 - recommendations['Discount'])                                                        # Price sensitivity\n",
        "    )\n",
        "    \n",
        "    # Get categories the customer hasn't purchased but are popular in their region\n",
        "    purchased_categories = set(zip(category_preferences['Category'], category_preferences['Sub Category']))\n",
        "    new_recommendations = regional_preferences[\n",
        "        ~regional_preferences.set_index(['Category', 'Sub Category']).index.isin(purchased_categories)\n",
        "    ].sort_values('Regional_Popularity', ascending=False)\n",
        "    \n",
        "    # Sort recommendations by score\n",
        "    recommendations = recommendations.sort_values('Score', ascending=False)\n",
        "    \n",
        "    return {\n",
        "        'top_recommendations': recommendations[['Category', 'Sub Category', 'Score']].head(5),\n",
        "        'new_categories': new_recommendations[['Category', 'Sub Category', 'Regional_Popularity']].head(3),\n",
        "        'customer_region': customer_region,\n",
        "        'customer_city': customer_city,\n",
        "        'purchase_history': category_preferences[['Category', 'Sub Category', 'Purchase_Frequency', \n",
        "                                               'Avg_Spend', 'Discount']],\n",
        "        'regional_trends': regional_preferences[['Category', 'Sub Category', 'Regional_Popularity', \n",
        "                                              'Customer_Base']]\n",
        "    }\n",
        "\n",
        "def display_recommendations(recommendations):\n",
        "    \"\"\"\n",
        "    Display the recommendations in a formatted way\n",
        "    \"\"\"\n",
        "    if 'error' in recommendations:\n",
        "        print(f\"Error: {recommendations['error']}\")\n",
        "        return\n",
        "        \n",
        "    print(f\"\\nPersonalized Recommendations for Customer in {recommendations['customer_city']}, {recommendations['customer_region']}\")\n",
        "    print(\"\\n1. Top 5 Recommended Categories Based on Your Shopping Pattern:\")\n",
        "    print(\"=\" * 80)\n",
        "    for idx, row in recommendations['top_recommendations'].iterrows():\n",
        "        print(f\"{idx + 1}. Category: {row['Category']:<20} Sub-Category: {row['Sub Category']:<20}\")\n",
        "        print(f\"   Recommendation Score: {row['Score']:.2f}\")\n",
        "    \n",
        "    print(\"\\n2. New Categories Popular in Your Region:\")\n",
        "    print(\"=\" * 80)\n",
        "    for idx, row in recommendations['new_categories'].iterrows():\n",
        "        print(f\"{idx + 1}. Category: {row['Category']:<20} Sub-Category: {row['Sub Category']:<20}\")\n",
        "        print(f\"   Regional Orders: {int(row['Regional_Popularity'])}\")\n",
        "    \n",
        "    print(\"\\n3. Your Shopping Pattern Summary:\")\n",
        "    print(\"=\" * 80)\n",
        "    history = recommendations['purchase_history']\n",
        "    for idx, row in history.iterrows():\n",
        "        print(f\"Category: {row['Category']:<20} Sub-Category: {row['Sub Category']:<20}\")\n",
        "        print(f\"Orders: {int(row['Purchase_Frequency']):>3}, Average Spend: ₹{row['Avg_Spend']:,.2f}, \" \\\n",
        "              f\"Typical Discount: {row['Discount']*100:.1f}%\")\n",
        "\n",
        "# Example usage:\n",
        "results = generate_personalized_recommendations(df, customer_name='Sudha')\n",
        "display_recommendations(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
